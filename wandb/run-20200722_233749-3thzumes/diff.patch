diff --git a/text_classification/config.py b/text_classification/config.py
index 9012fd7..dbbca68 100644
--- a/text_classification/config.py
+++ b/text_classification/config.py
@@ -1,20 +1,27 @@
 import os
+import sys
+
+sys.path.append(".")
+
 import logging
 import logging.config
 
-import utils
+from text_classification import utils
 
 # Directories
 BASE_DIR = os.getcwd()  # project root
+APP_DIR = os.path.dirname(__file__)  # app root
+
 LOGS_DIR = os.path.join(BASE_DIR, 'logs')
-EXPERIMENTS_DIR = os.path.join(BASE_DIR, 'experiments')
+EMBEDDINGS_DIR = os.path.join(BASE_DIR, 'embeddings')
 
 # Create dirs
 utils.create_dirs(LOGS_DIR)
-utils.create_dirs(EXPERIMENTS_DIR)
+utils.create_dirs(EMBEDDINGS_DIR)
 
 # Loggers
 log_config = utils.load_json(
     filepath=os.path.join(BASE_DIR, 'logging.json'))
+
 logging.config.dictConfig(log_config)
 logger = logging.getLogger('logger')
diff --git a/text_classification/data.py b/text_classification/data.py
index d385344..faf0144 100644
--- a/text_classification/data.py
+++ b/text_classification/data.py
@@ -25,16 +25,23 @@ ssl._create_default_https_context = ssl._create_unverified_context
 
 def load_data(url, data_size):
     """Load dataset from URL."""
-    df = pd.read_csv(url)
+
+    # \\ For CSV files
+    # df = pd.read_csv(url)
+
+    # For text files
+    df = pd.read_csv(url, sep="\t", header=None, names=['text', 'tag'])
+    df.dropna(inplace=True)
+
     df = df.sample(frac=1).reset_index(drop=True) # shuffle
 
     # Reduce dataset
-    # You should always overfit your models on a small
-    # dataset first so you can catch errors quickly.
+    # Always overfit your models on a small dataset first to catch errors quickly.
     df = df[:int(len(df)*data_size)]
 
-    X = df['title'].values
-    y = df['category'].values
+    X = df['text'].values
+    y = df['tag'].values
+
     return X, y
 
 
@@ -45,23 +52,23 @@ def preprocess_texts(texts, binary=True, lower=True, filters=r"[!\"'#$%&()*\+,-.
         if lower:
             text = text.lower()
 
-        # binary classification
-        if binary:
-        	text = re.match(r'\b\w{1,2}\b', text)
-
         # remove items text in () ex. (Reuters)
         # may want to refine to only remove if at end of text
         text = re.sub(r'\([^)]*\)', '', text)
 
-        # remove non-alphabetical letters
-        text = re.match('[^a-zA-Z]', text)
-
         # spacing and filters
         text = re.sub(r"([.,!?])", r" \1 ", text)
         text = re.sub(filters, r"", text)
         text = re.sub(' +', ' ', text)  # remove multiple spaces
         text = text.strip()
 
+        # remove non-alphabetical chars
+        # text = re.compile(r'[^a-zA-Z]')
+
+        # # binary classification
+        # if binary:
+        # 	text = re.compile(r'\b\w{1,2}\b')
+
         preprocessed_texts.append(text)
     return preprocessed_texts
 
@@ -179,9 +186,48 @@ def pad_sequences(sequences, max_seq_len=0):
     for i, sequence in enumerate(sequences):
         padded_sequences[i][:len(sequence)] = sequence
     return padded_sequences
+    
+
+class Model_LSTM_Dataset(torch.utils.data.Dataset):
+    def __init__(self, X, y):
+        self.X = X
+        self.y = y
+
+    def __len__(self):
+        return len(self.y)
+
+    def __str__(self):
+        return f"<Dataset(N={len(self)})>"
+
+    def __getitem__(self, index):
+        X = self.X[index]
+        y = self.y[index]
+        return X, y
+
+    def collate_fn(self, batch):
+        """Processing on a batch."""
+        # Get inputs
+        X = np.array(batch)[:, 0]
+        y = np.array(batch)[:, 1]
+
+        # Pad inputs
+        X = pad_sequences(sequences=X)
+
+        # Cast
+        X = torch.LongTensor(X.astype(np.int32))
+        y = torch.LongTensor(y.astype(np.int32))
+
+        return X, y
+
+    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):
+        return torch.utils.data.DataLoader(
+            dataset=self,
+            batch_size=batch_size,
+            collate_fn=self.collate_fn,
+            shuffle=shuffle, drop_last=drop_last, pin_memory=True)
 
 
-class TextDataset(torch.utils.data.Dataset):
+class Text_CNN_Dataset(torch.utils.data.Dataset):
     def __init__(self, X, y, max_filter_size):
         self.X = X
         self.y = y
diff --git a/text_classification/models.py b/text_classification/models.py
index 41f7357..b113b48 100644
--- a/text_classification/models.py
+++ b/text_classification/models.py
@@ -1,15 +1,19 @@
 # models.py - define model architectures.
 
+import math
+
 import torch
 import torch.nn as nn
+import torch.nn.functional as F
 
 from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
 
 
 class ModelLSTM(nn.Module):
     
-    def __init__(self, embedding_dim, vocab_size, hidden_dim, dropout_p,
-    			 num_classes, stacked_layers, padding_idx=0):
+    def __init__(self, embedding_dim, vocab_size, hidden_dim, stacked_layers, 
+    			 dropout_p, num_classes, pretrained_embeddings=None, 
+    			 freeze_embeddings=False, padding_idx=0):
         super(ModelLSTM, self).__init__()
 
          # Initialize embeddings
@@ -29,12 +33,12 @@ class ModelLSTM(nn.Module):
             self.embeddings.weight.requires_grad = False
         
         self.word_embeddings = nn.Embedding(num_embeddings = vocab_size, 
-        									embedding_dim = embedding_dim)
-        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_dim= hidden_dim, 
-        					batch_first = True, num_layers = stacked_layers, 
-        					dropout_p = dropout_p)
+                          	   				embedding_dim = embedding_dim)
+        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_dim, 
+		                    batch_first = True, num_layers = stacked_layers, 
+		                    dropout = dropout_p)
         self.linear = nn.Linear(in_features = hidden_dim, out_features=1)
-        self.tanh = nn.Tanh()
+        self.sigmoid = nn.Sigmoid()
         
     def forward(self, x_batch):
         len_list = list(map(len, x_batch))
@@ -42,11 +46,82 @@ class ModelLSTM(nn.Module):
         padded_batch = pad_sequence(x_batch, batch_first=True)
         embeds = self.word_embeddings(padded_batch)
         pack_embeds = pack_padded_sequence(embeds, lengths=len_list, 
-        								   batch_first=True, 
-        								   enforce_sorted=False)
+                           batch_first=True, 
+                           enforce_sorted=False)
         
         rnn_out, (rnn_h, rnn_c) = self.lstm(pack_embeds)
-        linear_out = self.linear(self.tanh(rnn_h))
+        linear_out = self.linear(self.sigmoid(rnn_h))
         y_out = linear_out[-1]
         
-        return y_out
\ No newline at end of file
+        return y_out
+
+
+class TextCNN(nn.Module):
+    def __init__(self, embedding_dim, vocab_size, num_filters, filter_sizes,
+                 hidden_dim, dropout_p, num_classes, pretrained_embeddings=None,
+                 freeze_embeddings=False, padding_idx=0):
+        super(TextCNN, self).__init__()
+
+        # Initialize embeddings
+        if pretrained_embeddings is None:
+            self.embeddings = nn.Embedding(
+                embedding_dim=embedding_dim, num_embeddings=vocab_size,
+                padding_idx=padding_idx)
+        else:
+            pretrained_embeddings = torch.from_numpy(
+                pretrained_embeddings).float()
+            self.embeddings = nn.Embedding(
+                embedding_dim=embedding_dim, num_embeddings=vocab_size,
+                padding_idx=padding_idx, _weight=pretrained_embeddings)
+
+        # Freeze embeddings or not
+        if freeze_embeddings:
+            self.embeddings.weight.requires_grad = False
+
+        # Conv weights
+        self.filter_sizes = filter_sizes
+        self.conv = nn.ModuleList(
+            [nn.Conv1d(in_channels=embedding_dim,
+                       out_channels=num_filters,
+                       kernel_size=f) for f in filter_sizes])
+
+        # FC weights
+        self.dropout = nn.Dropout(dropout_p)
+        self.fc1 = nn.Linear(num_filters*len(filter_sizes), hidden_dim)
+        self.fc2 = nn.Linear(hidden_dim, num_classes)
+
+    def forward(self, x_in, channel_first=False):
+
+        # Embed
+        x_in = self.embeddings(x_in)
+        if not channel_first:
+            x_in = x_in.transpose(1, 2)  # (N, channels, sequence length)
+
+        z = []
+        conv_outputs = []  # for interpretability
+        max_seq_len = x_in.shape[2]
+        for i, f in enumerate(self.filter_sizes):
+
+            # `SAME` padding
+            padding_left = int(
+                (self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2)
+            padding_right = int(math.ceil(
+                (self.conv[i].stride[0]*(max_seq_len-1) - max_seq_len + self.filter_sizes[i])/2))
+
+            # Conv
+            _z = self.conv[i](F.pad(x_in, (padding_left, padding_right)))
+            conv_outputs.append(_z)
+
+            # Pool
+            _z = F.max_pool1d(_z, _z.size(2)).squeeze(2)
+            z.append(_z)
+
+        # Concat outputs
+        z = torch.cat(z, 1)
+
+        # FC
+        z = self.fc1(z)
+        z = self.dropout(z)
+        logits = self.fc2(z)
+
+        return conv_outputs, logits
\ No newline at end of file
diff --git a/text_classification/predict.py b/text_classification/predict.py
index a6b09e8..e331464 100644
--- a/text_classification/predict.py
+++ b/text_classification/predict.py
@@ -32,17 +32,17 @@ def get_run_components(run_dir):
         fp=os.path.join(run_dir, 'y_tokenizer.json'))
 
     # Load ModelLSTM model
-    model = models.ModelLSTM(
-    	embedding_dim=args.embedding_dim, vocab_size=len(X_tokenizer)+1, 
-    	hidden_dim=args.hidden_dim, stacked_layers=args.stacked_layers,
-    	dropout_p=args.dropout_p, num_classes=len(y_tokenizer.classes))
+    # model = models.ModelLSTM(
+    # 	embedding_dim=args.embedding_dim, vocab_size=len(X_tokenizer)+1, 
+    # 	hidden_dim=args.hidden_dim, stacked_layers=args.stacked_layers,
+    # 	dropout_p=args.dropout_p, num_classes=len(y_tokenizer.classes))
 
     # // Load TextCNN model
-    # model = models.TextCNN(
-    #     embedding_dim=args.embedding_dim, vocab_size=len(X_tokenizer)+1,
-    #     num_filters=args.num_filters, filter_sizes=args.filter_sizes,
-    #     hidden_dim=args.hidden_dim, dropout_p=args.dropout_p,
-    #     num_classes=len(y_tokenizer.classes))
+    model = models.TextCNN(
+        embedding_dim=args.embedding_dim, vocab_size=len(X_tokenizer)+1,
+        num_filters=args.num_filters, filter_sizes=args.filter_sizes,
+        hidden_dim=args.hidden_dim, dropout_p=args.dropout_p,
+        num_classes=len(y_tokenizer.classes))
 
     model.load_state_dict(torch.load(os.path.join(run_dir, 'model.pt')))
 
@@ -108,7 +108,7 @@ def predict_step(model, dataloader, filter_sizes, device):
             # y_prob = F.softmax(logits, dim=1)
 
             # For binary instances
-            y_prob = F.sigmoid(logits, dim=1)
+            y_prob = F.sigmoid(logits)
 
             # Save probabilities
             y_probs.extend(y_prob.cpu().numpy())
@@ -133,7 +133,7 @@ def predict(inputs, args, model, X_tokenizer, y_tokenizer):
     X = np.array(X_tokenizer.texts_to_sequences(preprocessed_texts))
     y_filler = np.array([0]*len(X))
 
-    dataset = data.TextDataset(
+    dataset = data.Text_CNN_Dataset(
         X=X, y=y_filler, max_filter_size=max(args.filter_sizes))
     dataloader = dataset.create_dataloader(
         batch_size=args.batch_size)
diff --git a/text_classification/train.py b/text_classification/train.py
index 41fa365..339e69e 100644
--- a/text_classification/train.py
+++ b/text_classification/train.py
@@ -32,6 +32,17 @@ import torch.nn.functional as F
 from text_classification import config, data, models, utils
 
 
+def binary_acc(y_pred, y_test):
+    y_pred_tag = torch.round(torch.sigmoid(y_pred))
+
+    correct_results_sum = (y_pred_tag == y_test).sum().float()
+
+    acc = correct_results_sum / y_test.shape[0]
+    acc = torch.round(acc * 100)
+
+    return acc
+
+
 def train_step(model, optimizer, dataloader, device):
     """Train step."""
     # Set model to train mode
@@ -40,6 +51,7 @@ def train_step(model, optimizer, dataloader, device):
 
     # Iterate over train batches
     num_batches = len(dataloader)
+
     for i, (X, y) in tqdm(enumerate(dataloader), total=num_batches):
 
         # Step
@@ -64,11 +76,13 @@ def test_step(model, dataloader, device):
     """Validation or test step."""
     # Set model to eval mode
     model.eval()
+
     loss, correct = 0., 0
     y_preds, y_targets = [], []
 
     # Iterate over val batches
     num_batches = len(dataloader)
+
     with torch.no_grad():
         for i, (X, y) in enumerate(dataloader):
 
@@ -93,31 +107,34 @@ def test_step(model, dataloader, device):
 
 def train(model, optimizer, scheduler, num_epochs, patience,
           train_dataloader, val_dataloader, test_dataloader, device):
+
     best_val_loss = np.inf
     config.logger.info("Training:")
+
     for epoch in range(num_epochs):
         # Steps
         train_loss, train_acc = train_step(model, optimizer, train_dataloader, device)
-        val_loss, val_acc, _, _ = test_step(model, val_dataloader, device)
+        test_loss, test_acc, _, _ = test_step(model, val_dataloader, device)
 
-        # Metrics
-        config.logger.info(
-            f"Epoch: {epoch+1} | "
-            f"train_loss: {train_loss:.2f}, train_acc: {train_acc:.1f}, "
-            f"val_loss: {val_loss:.2f}, val_acc: {val_acc:.1f}")
-        wandb.log({
-            "train_loss": train_loss,
-            "train_accuracy": train_acc,
-            "val_loss": val_loss,
-            "val_accuracy": val_acc})
+        # print(f'Epoch {epoch+0:03}: | Train Loss: {train_loss / len(train_dataloader):.5f} | Train Acc: {train_acc} | Val Loss: {test_loss / len(val_dataloader):.5f} | Val Acc: {test_acc}')
 
         # Adjust learning rate
-        scheduler.step(val_loss)
+        scheduler.step(test_loss)
 
         # Early stopping
-        if val_loss < best_val_loss:
-            best_val_loss = val_loss
+        if test_loss < best_val_loss:
+            best_val_loss = test_loss
             _patience = patience  # reset _patience
+            # Metrics
+            config.logger.info(
+                f"Epoch: {epoch+1} | "
+                f"train_loss: {train_loss:.2f}, train_acc: {train_acc:.1f}, "
+                f"val_loss: {test_loss:.2f}, val_acc: {test_acc:.1f}")
+            wandb.log({
+                "train_loss": train_loss,
+                "train_accuracy": train_acc,
+                "val_loss": test_loss,
+                "val_accuracy": test_acc})
             torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'model.pt'))
         else:
             _patience -= 1
@@ -153,6 +170,7 @@ def plot_confusion_matrix(y_pred, y_target, classes, fp, cmap=plt.cm.Blues):
 
     # Values
     thresh = cm.max() / 2.
+
     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
         plt.text(j, i, f"{cm[i, j]:d} ({cm_norm[i, j]*100:.1f}%)",
                  horizontalalignment="center",
@@ -215,7 +233,7 @@ if __name__ == '__main__':
     parser.add_argument('--test-size', type=float,
                         default=0.15, help="test data proportion")
     parser.add_argument('--num-epochs', type=int,
-                        default=10, help="# of epochs to train")
+                        default=100, help="# of epochs to train")
     parser.add_argument('--batch-size', type=int, default=64,
                         help="# of samples per batch")
     parser.add_argument('--embedding-dim', type=int,
@@ -231,12 +249,12 @@ if __name__ == '__main__':
                         help="# of filters per cnn filter size")
     parser.add_argument('--hidden-dim', type=int, default=128,
                         help="# of hidden units in fc dense layers")
-    parser.add_argument('--stacked-layers', type=int, default=8,
-                        help="# of stacked lstm layers")
-    parser.add_argument('--dropout-p', type=float, default=0.1,
+    # parser.add_argument('--stacked-layers', type=int, default=8,
+    #                     help="# of stacked lstm layers")
+    parser.add_argument('--dropout-p', type=float, default=0.2,
                         help="dropout proportion in fc dense layers")
     parser.add_argument('--learning-rate', type=float,
-                        default=1e-3, help="initial learning rate")
+                        default=1e-4, help="initial learning rate")
     parser.add_argument('--patience', type=int, default=3,
                         help="# of epochs of continued performance regression")
     
@@ -270,7 +288,7 @@ if __name__ == '__main__':
     # Preprocesss
     original_X = X
     X = data.preprocess_texts(texts=X, binary=args.binary, 
-    						  lower=args.lower, filters=args.filters)
+                              lower=args.lower, filters=args.filters)
     
     config.logger.info(
         "Preprocessed data:\n"
@@ -323,6 +341,7 @@ if __name__ == '__main__':
 
     # Convert labels to tokens
     class_ = y_train[0]
+
     y_train = y_tokenizer.transform(y_train)
     y_val = y_tokenizer.transform(y_val)
     y_test = y_tokenizer.transform(y_test)
@@ -342,11 +361,17 @@ if __name__ == '__main__':
         f"  {class_weights}")
 
     # Create datasets
-    train_dataset = data.TextDataset(
+    # For ModelLSTM architecture
+    # train_dataset = data.Model_LSTM_Dataset(X=X_train, y=y_train)
+    # val_dataset = data.Model_LSTM_Dataset(X=X_val, y=y_val)
+    # test_dataset = data.Model_LSTM_Dataset(X=X_test, y=y_test)
+
+    # // For TextCNN architecture
+    train_dataset = data.Text_CNN_Dataset(
         X=X_train, y=y_train, max_filter_size=max(args.filter_sizes))
-    val_dataset = data.TextDataset(
+    val_dataset = data.Text_CNN_Dataset(
         X=X_val, y=y_val, max_filter_size=max(args.filter_sizes))
-    test_dataset = data.TextDataset(
+    test_dataset = data.Text_CNN_Dataset(
         X=X_test, y=y_test, max_filter_size=max(args.filter_sizes))
     
     config.logger.info(
@@ -358,12 +383,10 @@ if __name__ == '__main__':
         f"  {train_dataset[0]}")
 
     # Create dataloaders
-    train_dataloader = train_dataset.create_dataloader(
-        batch_size=args.batch_size)
-    val_dataloader = val_dataset.create_dataloader(
-        batch_size=args.batch_size)
-    test_dataloader = test_dataset.create_dataloader(
-        batch_size=args.batch_size)
+    train_dataloader = train_dataset.create_dataloader(batch_size=args.batch_size)
+    val_dataloader = val_dataset.create_dataloader(batch_size=args.batch_size)
+    test_dataloader = test_dataset.create_dataloader(batch_size=args.batch_size)
+
     batch_X, batch_y = next(iter(train_dataloader))
     
     config.logger.info(
@@ -373,6 +396,7 @@ if __name__ == '__main__':
 
     # Load embeddings
     embedding_matrix = None
+
     if args.use_glove:
         if args.embedding_dim not in (50, 100, 200, 300):
             raise Exception(
@@ -389,28 +413,28 @@ if __name__ == '__main__':
             f"{embedding_matrix.shape}")
 
     # ModelLSTM Artchitecture
-    model = models.ModelLSTM(
-    	embedding_dim=args.embedding_dim, 
-    	vocab_size=vocab_size, 
-    	hidden_dim=args.hidden_dim, 
-    	stacked_layers=stacked_layers,
-    	dropout_p=args.dropout_p,
-    	num_classes=len(y_tokenizer.classes), 
-    	pretrained_embeddings=embedding_matrix,
-        freeze_embeddings=args.freeze_embeddings)
-
-   	# // TextCNN Architecture
-    # model = models.TextCNN(
+    # model = models.ModelLSTM(
     #     embedding_dim=args.embedding_dim, 
-    #     vocab_size=vocab_size,
-    #     num_filters=args.num_filters, 
-    #     filter_sizes=args.filter_sizes,
+    #     vocab_size=vocab_size, 
     #     hidden_dim=args.hidden_dim, 
+    #     stacked_layers=args.stacked_layers,
     #     dropout_p=args.dropout_p,
-    #     num_classes=len(y_tokenizer.classes),
+    #     num_classes=len(y_tokenizer.classes), 
     #     pretrained_embeddings=embedding_matrix,
     #     freeze_embeddings=args.freeze_embeddings)
 
+       # // TextCNN Architecture
+    model = models.TextCNN(
+        embedding_dim=args.embedding_dim, 
+        vocab_size=vocab_size,
+        num_filters=args.num_filters, 
+        filter_sizes=args.filter_sizes,
+        hidden_dim=args.hidden_dim, 
+        dropout_p=args.dropout_p,
+        num_classes=len(y_tokenizer.classes),
+        pretrained_embeddings=embedding_matrix,
+        freeze_embeddings=args.freeze_embeddings)
+
     model = model.to(device)
     wandb.watch(model)
     
@@ -444,16 +468,16 @@ if __name__ == '__main__':
         "test_accuracy": test_acc})
 
     # Per-class performance analysis
-    performance = get_performance(y_pred, y_target, classes)
+    # performance = get_performance(y_pred y_target, classes)
 
-    plot_confusion_matrix(
-        y_pred=y_pred, y_target=y_target, classes=classes,
-        fp=os.path.join(wandb.run.dir, 'confusion_matrix.png'))
+    # plot_confusion_matrix(
+    #     y_pred=y_pred, y_target=y_target, classes=classes,
+    #     fp=os.path.join(wandb.run.dir, 'confusion_matrix.png'))
 
-    utils.save_dict(performance, filepath=os.path.join(
-        wandb.run.dir, 'performance.json'))
+    # utils.save_dict(performance, filepath=os.path.join(
+    #     wandb.run.dir, 'performance.json'))
     
-    config.logger.info(json.dumps(performance, indent=2, sort_keys=False))
+    # config.logger.info(json.dumps(performance, indent=2, sort_keys=False))
 
     # Save
     utils.save_dict(args.__dict__, filepath=os.path.join(
